{
  
    
        "post0": {
            "title": "Lemonaderecipedocx",
            "content": "LEMONADE RECIPE . Step 1: Ingredients: 3 ripe lemons (they need to be a little soft!) … . Step 2: Juice Your Lemons. You need between 1/2 and 2/3 cup of lemon juice. … . Step 3: Make the Simple Syrup. Pour one cup of sugar into your glass measuring cup and add one cup of water on top. … . Step 4: Mix and Serve! .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/2023/03/12/LemonadeRecipeDocx.html",
            "relUrl": "/2023/03/12/LemonadeRecipeDocx.html",
            "date": " • Mar 12, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd # importing pandas . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . Column 3 row 5 &quot;junior&quot; | Column 3 row 9 &quot;9th grade&quot; | column 2 row 10 &quot;nil&quot; | . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . DataFrame Sort . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd #the data can be stored as a python dictionary dict = { &quot;calories&quot;: [420, 380, 390], &quot;duration&quot;: [50, 40, 45] } #stores the data in a data frame print(&quot;-- --Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) #or with the index argument, you can label rows. df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;]) print(df) . -- --Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 3 entries, day1 to day3 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 3 non-null int64 1 duration 3 non-null int64 dtypes: int64(2) memory usage: 180.0+ bytes None . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . country_name cases deaths 0 USA 82,649,779 1,018,316 1 India 43,057,545 522,193 2 Brazil 30,345,654 662,663 3 France 28,244,977 145,020 4 Germany 24,109,433 134,624 5 UK 21,933,206 173,352 . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Fambulous. | . When choosing a data set, think about the following:- Does it have a good sample size? - Is there bias in the data? - Does the data set need to be cleaned? - What is the purpose of the data set? - ... . Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. Here are some suggestions:- Titanic Dataset: This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. . Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset:- Kaggle: The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. . UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning . From Tutorials Point%20is,a%20consistence%20interface%20in%20Python). Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.&gt; Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. Decision Trees, prediction by a piecewise constant approximation. - Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . DecisionTreeClassifier Accuracy: 0.7588235294117647 LogisticRegression Accuracy: 0.788235294117647 . /Users/kushsirohi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . NBA PLAYER Dataset . Here is a dataset that I found online, which displays the stats of nba players . import pandas as pd df = pd.read_csv(&#39;files/NBA.csv&#39;).sort_values(by=[&#39;age&#39;], ascending=False) print(&quot;--FG Top 10&quot;) print(df.head(10)) # Ask the teacher the question. Df.head only works when using it in this cell block, but when I add df.tail, it doesnt show. I have to put it in a different cell block for it to show. . --FG Top 10 Unnamed: 0 player_name team_abbreviation age player_height 4698 4698 Kevin Willis DAL 44.0 213.36 270 270 Robert Parish CHI 43.0 215.90 10818 10818 Vince Carter ATL 43.0 198.12 5680 5680 Dikembe Mutombo HOU 43.0 218.44 4892 4892 Dikembe Mutombo HOU 42.0 218.44 12149 12149 Udonis Haslem MIA 42.0 203.20 10240 10240 Vince Carter ATL 42.0 198.12 3795 3795 Kevin Willis ATL 42.0 213.36 1291 1291 Herb Williams NYK 41.0 210.82 2818 2818 John Stockton UTA 41.0 185.42 player_weight college country draft_year draft_round ... 4698 111.130040 Michigan State USA 1984 1 ... 270 110.676448 Centenary (LA) USA 1976 1 ... 10818 99.790240 North Carolina USA 1998 1 ... 5680 117.933920 Georgetown Congo 1991 1 ... 4892 117.933920 Georgetown Congo 1991 1 ... 12149 106.594120 Florida USA Undrafted Undrafted ... 10240 99.790240 North Carolina USA 1998 1 ... 3795 111.130040 Michigan State USA 1984 1 ... 1291 117.933920 Ohio State USA 1981 1 ... 2818 79.378600 Gonzaga USA 1984 1 ... pts reb ast net_rating oreb_pct dreb_pct usg_pct ts_pct 4698 2.4 1.6 0.2 11.9 0.063 0.222 0.184 0.432 270 3.7 2.1 0.5 14.9 0.114 0.130 0.201 0.514 10818 5.0 2.1 0.8 -8.1 0.019 0.114 0.163 0.470 5680 1.8 3.7 0.0 1.8 0.152 0.241 0.111 0.472 4892 3.0 5.1 0.1 3.6 0.118 0.221 0.086 0.591 12149 2.5 1.9 0.3 -9.5 0.043 0.239 0.183 0.495 10240 7.4 2.6 1.1 -6.3 0.021 0.114 0.164 0.565 3795 3.0 2.6 0.3 -15.5 0.091 0.164 0.151 0.434 1291 1.7 1.0 0.0 0.9 0.088 0.103 0.138 0.563 2818 10.8 2.5 7.7 5.2 0.028 0.078 0.191 0.577 ast_pct season 4698 0.034 2006-07 270 0.082 1996-97 10818 0.081 2019-20 5680 0.000 2008-09 4892 0.012 2007-08 12149 0.077 2021-22 10240 0.097 2018-19 3795 0.043 2004-05 1291 0.000 1998-99 2818 0.445 2002-03 [10 rows x 22 columns] . import pandas as pd df = pd.read_csv(&#39;files/NBA.csv&#39;).sort_values(by=[&#39;age&#39;], ascending=False) print(&quot;--bottom 10&quot;) print(df.tail(10)) . --bottom 10 Unnamed: 0 player_name team_abbreviation age player_height 8488 8488 Bruno Caboclo TOR 19.0 205.74 8783 8783 Rashad Vaughn MIL 19.0 198.12 12194 12194 Joshua Primo SAS 19.0 193.04 4410 4410 Martell Webster POR 19.0 200.66 9206 9206 Marquese Chriss PHX 19.0 208.28 10523 10523 Kevin Knox II NYK 19.0 205.74 11829 11829 Jaden Springer PHI 19.0 193.04 342 342 Kobe Bryant LAL 18.0 200.66 78 78 Jermaine O&#39;Neal POR 18.0 210.82 4138 4138 Andrew Bynum LAL 18.0 213.36 player_weight college country draft_year draft_round ... pts 8488 92.986360 None Brazil 2014 1 ... 1.3 8783 91.625584 UNLV USA 2015 1 ... 3.1 12194 85.728888 Alabama Canada 2021 1 ... 5.8 4410 95.254320 None USA 2005 1 ... 6.6 9206 105.686936 Washington USA 2016 1 ... 9.2 10523 97.522280 Kentucky USA 2018 1 ... 12.8 11829 91.625584 Tennessee USA 2021 1 ... 1.0 342 90.718400 None USA 1996 1 ... 7.6 78 102.511792 None USA 1996 1 ... 4.1 4138 124.737800 None USA 2005 1 ... 1.6 reb ast net_rating oreb_pct dreb_pct usg_pct ts_pct ast_pct 8488 0.3 0.0 -60.3 0.048 0.050 0.315 0.417 0.000 8783 1.3 0.6 -6.8 0.012 0.086 0.133 0.398 0.055 12194 2.3 1.6 -3.3 0.031 0.079 0.152 0.487 0.111 4410 2.1 0.6 -14.6 0.029 0.120 0.183 0.533 0.055 9206 4.2 0.7 -10.8 0.058 0.164 0.196 0.529 0.051 10523 4.5 1.1 -13.6 0.025 0.120 0.219 0.475 0.060 11829 1.0 0.0 -72.4 0.143 0.125 0.118 1.000 0.000 342 1.9 1.3 2.2 0.048 0.084 0.244 0.544 0.144 78 2.8 0.2 1.3 0.099 0.198 0.199 0.494 0.030 4138 1.7 0.2 -4.0 0.110 0.154 0.152 0.394 0.044 season 8488 2014-15 8783 2015-16 12194 2021-22 4410 2005-06 9206 2016-17 10523 2018-19 11829 2021-22 342 1996-97 78 1996-97 4138 2005-06 [10 rows x 22 columns] . print(&quot;Here are the players with the highest average points&quot;) print(df[df.pts == df.pts.max()]) print() print(&quot;Here are the players with the lowest average points&quot;) print(df[df.pts == df.pts.min()]) print() . Here are the players with the highest average points Unnamed: 0 player_name team_abbreviation age player_height 10572 10572 James Harden HOU 29.0 195.58 player_weight college country draft_year draft_round ... pts 10572 99.79024 Arizona State USA 2009 1 ... 36.1 reb ast net_rating oreb_pct dreb_pct usg_pct ts_pct ast_pct 10572 6.6 7.5 6.3 0.023 0.157 0.396 0.616 0.394 season 10572 2018-19 [1 rows x 22 columns] Here are the players with the lowest average points Unnamed: 0 player_name team_abbreviation age 3111 3111 Olden Polynice LAC 39.0 2676 2676 Chris Dudley POR 38.0 2090 2090 Muggsy Bogues TOR 36.0 6721 6721 Brian Skinner MEM 36.0 6279 6279 Brian Skinner MIL 35.0 ... ... ... ... ... 3263 3263 Slavko Vranes POR 21.0 11578 11578 Ashton Hagans MIN 21.0 2795 2795 Jamal Sampson MIL 20.0 10310 10310 Ike Anigbogu IND 20.0 4411 4411 Martynas Andriuskevicius CLE 20.0 player_height player_weight college country draft_year 3111 208.28 113.398000 Virginia USA 1987 2676 210.82 117.933920 Yale USA 1987 2090 160.02 61.688512 Wake Forest USA 1987 6721 205.74 115.665960 Baylor USA 1998 6279 205.74 115.665960 Baylor USA 1998 ... ... ... ... ... ... 3263 226.06 124.737800 None USA 2003 11578 190.50 86.182480 Kentucky USA Undrafted 2795 210.82 106.594120 California USA 2002 10310 208.28 113.398000 UCLA USA 2017 4411 218.44 108.862080 None Lithuania 2005 draft_round ... pts reb ast net_rating oreb_pct dreb_pct 3111 1 ... 0.0 1.0 0.5 1.8 0.111 0.100 2676 4 ... 0.0 0.7 0.0 -65.5 0.167 0.000 2090 1 ... 0.0 1.0 1.7 -27.4 0.000 0.120 6721 1 ... 0.0 0.0 0.0 -88.4 0.000 0.000 6279 1 ... 0.0 0.0 0.0 -95.6 0.000 0.000 ... ... ... ... ... ... ... ... ... 3263 2 ... 0.0 0.0 0.0 -10.0 0.000 0.000 11578 Undrafted ... 0.0 0.0 0.0 -21.8 0.000 0.000 2795 2 ... 0.0 0.4 0.2 12.3 0.125 0.125 10310 2 ... 0.0 1.0 0.3 -14.3 0.143 0.250 4411 2 ... 0.0 0.7 0.0 37.1 0.125 0.375 usg_pct ts_pct ast_pct season 3111 0.172 0.0 0.143 2003-04 2676 0.045 0.0 0.000 2002-03 2090 0.078 0.0 0.294 2000-01 6721 0.100 0.0 0.000 2011-12 6279 0.000 0.0 0.000 2010-11 ... ... ... ... ... 3263 0.145 0.0 0.000 2003-04 11578 0.083 0.0 0.000 2020-21 2795 0.118 0.0 0.333 2002-03 10310 0.267 0.0 0.200 2018-19 4411 0.042 0.0 0.000 2005-06 [156 rows x 22 columns] . print(&quot;Average overall stats of the typical player in the NBA&quot;) df.mean() . Average overall stats of the typical player in the NBA . /var/folders/t5/fkg7hq2x4ml17j36_f8wxr000000gn/T/ipykernel_24989/962595910.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. df.mean() . Unnamed: 0 6152.000000 age 27.084518 player_height 200.611602 player_weight 100.369926 gp 51.290532 pts 8.172775 reb 3.559155 ast 1.813986 net_rating -2.255733 oreb_pct 0.054473 dreb_pct 0.141014 usg_pct 0.184891 ts_pct 0.511060 ast_pct 0.131358 dtype: float64 . print(&quot;Here are the players with their average number of points over 20&quot;) print(df[df.pts &gt; 20.00]) . Here are the players with their average number of points over 20 Unnamed: 0 player_name team_abbreviation age player_height 2995 2995 Karl Malone UTA 39.0 205.74 2199 2199 Michael Jordan WAS 39.0 198.12 2276 2276 Karl Malone UTA 38.0 205.74 2079 2079 Karl Malone UTA 37.0 205.74 12175 12175 LeBron James LAL 37.0 205.74 ... ... ... ... ... ... 11365 11365 Zion Williamson NOP 20.0 200.66 10451 10451 Luka Doncic DAL 20.0 200.66 3120 3120 LeBron James CLE 19.0 203.20 10799 10799 Zion Williamson NOP 19.0 198.12 5208 5208 Kevin Durant SEA 19.0 205.74 player_weight college country draft_year draft_round ... 2995 116.119552 Louisiana Tech USA 1985 1 ... 2199 97.975872 North Carolina USA 1984 1 ... 2276 116.119552 Louisiana Tech USA 1985 1 ... 2079 116.119552 Louisiana Tech USA 1985 1 ... 12175 113.398000 None USA 2003 1 ... ... ... ... ... ... ... ... 11365 128.820128 Duke USA 2019 1 ... 10451 98.883056 None Slovenia 2018 1 ... 3120 108.862080 None USA 2003 1 ... 10799 128.820128 Duke USA 2019 1 ... 5208 97.522280 Texas USA 2007 1 ... pts reb ast net_rating oreb_pct dreb_pct usg_pct ts_pct 2995 20.6 7.8 4.7 3.0 0.049 0.203 0.276 0.534 2199 22.9 5.7 5.2 -0.3 0.027 0.163 0.361 0.468 2276 22.4 8.6 4.3 0.8 0.057 0.218 0.288 0.532 2079 23.2 8.3 4.5 5.8 0.050 0.236 0.299 0.572 12175 30.3 8.2 6.2 -2.5 0.031 0.185 0.317 0.619 ... ... ... ... ... ... ... ... ... 11365 27.0 7.2 3.7 2.1 0.081 0.131 0.287 0.649 10451 21.2 7.8 6.0 -3.1 0.036 0.198 0.296 0.545 3120 20.9 5.5 5.9 -2.6 0.035 0.117 0.280 0.488 10799 22.5 6.3 2.1 5.1 0.092 0.114 0.291 0.616 5208 20.3 4.4 2.4 -11.7 0.027 0.111 0.278 0.519 ast_pct season 2995 0.236 2002-03 2199 0.295 2001-02 2276 0.216 2001-02 2079 0.234 2000-01 12175 0.297 2021-22 ... ... ... 11365 0.188 2020-21 10451 0.317 2018-19 3120 0.268 2003-04 10799 0.119 2019-20 5208 0.120 2007-08 [646 rows x 22 columns] . print(&quot;These are the median stats by the players in the NBA &quot;) df.median() . These are the median stats by the players in the NBA . /var/folders/t5/fkg7hq2x4ml17j36_f8wxr000000gn/T/ipykernel_24989/1381966225.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with &#39;numeric_only=None&#39;) is deprecated; in a future version this will raise TypeError. Select only valid columns before calling the reduction. df.median() . Unnamed: 0 6152.00000 age 26.00000 player_height 200.66000 player_weight 99.79024 gp 57.00000 pts 6.70000 reb 3.00000 ast 1.20000 net_rating -1.30000 oreb_pct 0.04100 dreb_pct 0.13100 usg_pct 0.18100 ts_pct 0.52400 ast_pct 0.10300 dtype: float64 .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/collegeboard/2023/03/09/AP-unit2-3.html",
            "relUrl": "/collegeboard/2023/03/09/AP-unit2-3.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. - Clouds Impression . Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab | Size, height and width, number of pixels | Visual perception, lossy compression | . Python Libraries and Concepts used for Jupyter and Files/Directories . Introduction to displaying images in Jupyter notebook . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? | What are the command you use in Windows terminal to access files? | What are some of the major differences? | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? | How does the meta data source and label relate to Unit 5 topics? | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . Translate first 3 letters of your name to Base64. | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? | From your consumer experience, what effects have you experienced from buffering? | How do these effects apply to images? | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? | Describe Grey Scale algorithm in English or Pseudo code? | Describe scale image? What is before and after on pixels in three images? | Is scale image a type of compression? If so, line it up with College Board terms described? | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . - meta data -- Green Square Internet PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- original image -- . grey image - . - meta data -- Clouds Impression Peter Carolin PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- original image -- . grey image - . - meta data -- Lassen Volcano Peter Carolin JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- original image -- . grey image - . . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL | numpy | base64 | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- scaled image -- . grey image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . grey image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . grey image . . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . notes . 2.2 blog notes . lossy compression, remove some of the image itself, not fully scalabale to its original | lossless compression, make the image smaller but scalable to its original | . What are commands you use in terminal to access files . I use ls, cd, open, those are some the the commands I use on mac #### What are the command you use in Windows terminal to access files? | I dont use windows #### What are some of the major differences? | They are different operating systems, so I would image that they would have different commands and different paths | . Why is path a big deal when working with images? . my guess would be to locate the image on your computer to display on the project or blog ### How does the meta data source and label relate to Unit 5 topics? | My guess would be to store the image in the backend, or recieve user data from it. | . Translate first 3 letters of your name to Base64. . a3Vx | . BytesIO . Where have you been a consumer of buffering? . when im watching vides online, or when I am trying to load up a site on bad wifi From your consumer experience, what effects have you experienced from buffering? | It makes my user expereince less enjoyable How do these effects apply to images? | Im guessing it takes time for these images to load up | . PIL . numpy . base64 .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/jupyter/2023/03/07/AP-unit2.2.html",
            "relUrl": "/jupyter/2023/03/07/AP-unit2.2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": ". layout: default . layout: base . Unit 2.2 Data Compression, Images . Lab will perform alterations on images, manipulate RGB values, and reduce the number of pixels. College Board requires you to learn about Lossy and Lossless compression. . toc:true- image: /images/python.png | categories: [] | type: ap | week: 25 | . Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. - Clouds Impression . Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab | Size, height and width, number of pixels | Visual perception, lossy compression | . Python Libraries and Concepts used for Jupyter and Files/Directories . Introduction to displaying images in Jupyter notebook . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? | What are the command you use in Windows terminal to access files? | What are some of the major differences? | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? | How does the meta data source and label relate to Unit 5 topics? | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . Translate first 3 letters of your name to Base64. | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? | From your consumer experience, what effects have you experienced from buffering? | How do these effects apply to images? | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? | Describe Grey Scale algorithm in English or Pseudo code? | Describe scale image? What is before and after on pixels in three images? | Is scale image a type of compression? If so, line it up with College Board terms described? | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL | numpy | base64 | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/2023/03/07/AP-unit2-2.html",
            "relUrl": "/2023/03/07/AP-unit2-2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Docker server",
            "content": "",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/09/19/docker-server.html",
            "relUrl": "/markdown/2022/09/19/docker-server.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "DNHS BELL SCHEDULE",
            "content": "Monday, Tuesday, Thursday . Period Start Time End Time . Period 1 | 8:35 AM | 9:44 AM | . Period 2 | 9:49 AM | 10:58 AM | . Break | 10:58 AM | 11:08 AM | . Period 3 | 11:13 AM | 12:22 PM | . Lunch | 12:22 PM | 12:52 PM | . Period 4 | 12:57 PM | 2:06 PM | . Office Hours | 2:06 PM | 2:31 PM | . Period 5 | 2:36 PM | 3:45 PM | . Wednesday . Period Start Time End Time . Period 1 | 9:55 AM | 10:53 AM | . Period 2 | 10:58 AM | 11:56 AM | . Break | 11:56 AM | 12:06 pM | . Period 3 | 12:11 PM | 1:09 PM | . Lunch | 1:09 PM | 1:39 PM | . Period 4 | 1:44 PM | 2:42 PM | . Office Hours | 2:06 PM | 2:31 PM | . Period 5 | 2:47 PM | 3:45 PM | . Friday . Period Start Time End Time . Period 1 | 8:35 AM | 9:49 AM | . Period 2 | 9:54 AM | 11:08 AM | . Break | 11:08 AM | 11:18 AM | . Period 3 | 11:23 AM | 12:37 PM | . Lunch | 12:37 PM | 1:07 PM | . Period 4 | 1:12 PM | 2:26 PM | . Period 5 | 2:31 PM | 3:45 PM | . .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/09/05/schedule.html",
            "relUrl": "/markdown/2022/09/05/schedule.html",
            "date": " • Sep 5, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Collegeboard",
            "content": "COllEGE BOARD . . . . What I have been doing with the quizzes recently, is just looking at the code and try to find the most logical answer that I can get from the syntax shows. An example is, if, while, and for. I know what all these mean, but I dont know how to implement them into my own programming. With this quiz, it made me think more logically. The question I got wrong, I had trouble with the syntax, and thats why I didnt understand. In the future, I will study the syntax. .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/2022/09/05/collegeboard.html",
            "relUrl": "/2022/09/05/collegeboard.html",
            "date": " • Sep 5, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "PHYSICS NOTES",
            "content": "Velocity lab Purpose: . To understand the difference between displacement and position. | To calculate an average velocity. | To analyze average velocity by using a graph. | To extrapolate from a graph. | To see the relationship between displacement, time, and average speed. Process: Use chalk or blue tape to mark off 1 meter sections on the floor, up to 5 meters. Run the Tumble Buggy several times and decide how you are going to measure the start and stop times. | . | Use a stopwatch to measure the time it takes the Buggy to travel from 0 to 1 meters, 0 to 2 meters, 0 to 3 meters, 0 to 4 meters, and 0 to 5 meters. Do 3 trials for all distances and take an average time for each distance. Create a clean and easy-to-follow table to input all of the numbers from the above process. | Graph: Create a full-page graph with position (meters) on the y-axis and average time (seconds) on the x-axis. Label this the “slow car”. What would a car that moves twice as fast look like on this graph? Using another color, create another line on the same graph showing how a hypothetical “fast car” would travel if the “fast car” can go 2 times faster than the “slow car”. Using a third color, create another line on the same graph showing the “slow car” traveling in the opposite direction (have the car start at 5 m and move to 0 m). Label this “reverse car.” | Analysis: Calculate the slope of each line. What are the units associated with the slope? What does this imply about the meaning of the value of the slope? Does the slope change? What does this indicate about the motion of the car? Which car is associated with the greater slope? What general statement can be made that relates the slope of the best fit line to the speed of the car? Determine the time it took for the slow car to reach the 2.5 m mark. Where was slow car at the time t = 12.5 s? Based on the data you collected, how long would it take the fast car to get to a six meter mark? Where would the slow car be after 12 seconds? Using the relationships between change in position and the change in time, predict how long it would take the fast car to travel 45 m. What would be the change in the position of the slow car after 50 s? What would be the average speed of the slow car if it traveled from the zero to the five meter mark (use the time from your data table)? Average velocity? What would be the average speed of the slow car if it traveled from the zero to the five meter mark and back to the zero mark (double the time)? Average velocity? | . Acceleration Lab Analysis Questions . What are the units of the slope for your velocity vs time graph? What does the slope represent? . | The slope represents the acceleration of the object and the units are meters per second (m/s).  . | Describe the shape of the position vs time graphs. What does the shape of your position time graph tell you about the motion of the car? . | The shape represents velocity, which is displacement over time. The shape gives detail of the slope, which is the velocity of the position v. time.  . | What shape is the acceleration vs time graph? What does the shape of the graph tell you about the motion of the car? . | The shape is exponential because as the car changed its position downward, the velocity increased.  . | When did the car have the highest acceleration? What causes the cars to accelerate? . | The car had the highest acceleration at the end of the racetrack according to our graphs. What causes the acceleration was the change of velocity the cars experienced on the track depending on the height that they were at.  . | Compare your acceleration values from the kinematic section to the Motion Tracker section of your data. Which was likely more accurate? Explain your reasoning. . | The kinematics section was  likely more accurate. During our motion tracker, there were different variables, such as the slope of the table, the variance in length, and whatnot. Since the kinematics was an equation, I would say it’s more accurate.  . | Examine the velocity time graph below and answer the following. . | Describe the motion of a car that creates this graph. When is the car accelerating, when is its motion constant? . | The car accelerates, then moves with constant velocity, then accelerates, then stops moving altogether. A flat line on a velocity time graph represents a constant motion. A flat line on the x axis on a velocity time graph represents no motion. Lines on a velocity time graph with slopes that are not 0 represent motion. . | Calculate the acceleration from 0-14 seconds, 14-44 seconds and 44-54 seconds. . | 0-14: 1.92 m/s^2 14-44: 0 m/s^2 44-54: -4 m/s^2 . Sketch a velocity vs time graph that is speeding up in the negative direction. |",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/school/2022/08/28/PHYSICSNOTES.html",
            "relUrl": "/school/2022/08/28/PHYSICSNOTES.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "PHOTOGRAPHY NOTES",
            "content": ".",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/school/2022/08/28/PHOTOGRAPHY.html",
            "relUrl": "/school/2022/08/28/PHOTOGRAPHY.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "APCSP NOTES",
            "content": "week 0 . created a fastpages website | uploaded a jupyter file | uploaded a markdown file week 1 . | Worked on fastpages | Added more elements to my fastpages | created a calculator with python week 2 . | used html fragments | Added vocab | logged physics work inside fastpages | added more elements to my fastpages such as vocab and colors | parallax effect added | learned how to use tables week 3 . | Worked on app lab | made a quiz | logged my data in fastpages week 4 . | created a fastpages server | created a flask python server | made a video detailing both | .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/school/2022/08/28/APCSP.html",
            "relUrl": "/school/2022/08/28/APCSP.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "First Markdown Post",
            "content": "Markdown is a type of language that similar to html, in the aspect that they both are markup languages . print(&quot;You can also use python&quot;) . this | is a | ordered list | Big heading . Sorta big heading . Small heading .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/08/21/markdownpost-(1).html",
            "relUrl": "/markdown/2022/08/21/markdownpost-(1).html",
            "date": " • Aug 21, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "CB mcq 3",
            "content": "My score . Overall Score: 94% . . Overall Reflection . I found that when I spend more time analyzing each question, I do better on these types of quizzes. Every question that I got wrong werent anything too conceptual, but silly mistakes. | . . Question 22 . . . Answer explanation . Here I chose choice B, while the answer was choice A. You can see, the code iterates in a way that if the arrow can rotate right, it will rotate right. Then, if the arrow can rotate left, it will rotate left. Finally, if the arrow can rotate forward, it will rotate forward. With grid I, it will rotate forward until it hits the side, rotate right twice, and go down until it hits the middle row, then it will rotate right and go forward. After that, it will rotate left and go until it reaches the top row. After, it will rotate left until it hits the destination. . Why did I get this wrong . I simple terms, I rushed this question and did not evaluate option A. I evaluated option B and made an error in my judgement . . Question 33 . . Answer explanation and why I got it wrong . . Here, I chose choice C when the correct answer was choice A. The code segment moves it right, forward, left, forward, and right. It does that 4 times. When I was going thru the steps in my head, it seemed correct that it would go thru 3 times. When I looked at it again just now, I saw that it would only need to repeat twice and repeating it 3 times would leave it three rows above its starting position and one row above the gray square. . . Question 35 . . Answer explanation and why I got it wrong . I picked choice C, when instead I should have picked choice A. The reason I got this incorrect was that I rushed thru this question and did not consider the other choices. Choice A is correct, because if team A has more points than team B, they win. If the first condition is wrong, that mean team B has more points and if team B has more points than team C, they win. If with the else statement, if Team C has more points than A and B, then they win. .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/03/12/CBmcq3.html",
            "relUrl": "/markdown/2022/03/12/CBmcq3.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "5.5 5.6",
            "content": "Blog Post Reflection . When you create a GitHub repository it requests a license type. Review the license types in relationship to this Tech Talk and make some notes in your personal blog . For our repository, to make it open source, you must have a open source license, which we have included. . In your blog, summarize the discussions and personal analysis on Software Licenses/Options, Digital Rights, and other Legal and Ethical thoughts from this College Board topic. . If someone doesnt thoroughly read the license, they could agree sometihng that they do not agree with . Make a license for your personal (blog) and Team repositories for the CPT project. Be sure to have a license for both Team GitHub repositories (frontend/backend). Document license(s) you picked and why. FYI, frontend, since it is built on GitHub pages may come with a license and restrictions. Document in blog how team made license choice and process of update. . We have done so ` .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/02/13/bloggy.html",
            "relUrl": "/markdown/2022/02/13/bloggy.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "5.3 5.4",
            "content": "Google “What age groups use Facebook” vs “… TikTok”? What does the data say? Is there purposeful exclusion in these platforms? Is it harmful? Should it be corrected? Is it good business? . Facebook has an older age range, typically, the users of facebook are 24 - 35. . Why do virtual assistants have female voices? Amazon, Alexa Google, Apple Siri. Was this purposeful? Is it harmful? Should it be corrected? Is it good business? . Female voice can usually being trusted by human and humans have preference. I would not consider it harmful, because the same thing would go it it was a male saying it. . Talk about an algorithm that influences your decisions, think about these companies (ie FAANG - Facebook, Amazon, Apple,Netflix, Google) . Does the owner of the computer think this was intentional? Yes, he think this is intentional because he think it’s racist. . How do you think this happened? . The company didn’t run enough test to test this function. . Is this harmful? Was it intended to be harmful or exclude? . Yes, since it discriminates to a certain group of people . Should it be corrected? . Yes, this should be corrected. . What would you or should you do to produce a better outcome? . You an increase the sample size . We have all experienced Crowdsourcing by using external data through API’s, namely RapidAPI. This data has influenced how we code and shown possibilities in obtaining and analyzing data. Discuss APIs you have used. . We used a stock api to import stock data. . Describe a crowdsource idea and how you might initiate it in our environment? . People can contribute thru . . What about Del Norte crowdsourcing? Could your project be better with crowdsourcing? . Yes, the crowdsourcing can give more ideas on how to improve your project and how the project will have all the function working well. . What kind of data could you capture at N@tM to make evening interesting? Perhaps use this data to impress Teachers during finals week. . We can show survey data from interviewing people on our jpriject. .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/02/13/5.15.2blognotes.html",
            "relUrl": "/markdown/2022/02/13/5.15.2blognotes.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Project Blog",
            "content": "Future Feature: Learning Module . Program purpose and function . The purpose of the learning feature will make a module where users can learn about day trading, and in the end they could take a quiz that would . Data Abstraction . I would use lists, classes, dictionaries to hold the value for the quiz . Procedural Abstraction . I would use a function that allowed me to make a checkpoint, so when you resolve the help description, it would see that you read it and would complete the training module for that part. . Algorithm Implementation . I would create an algorithm that checks if the user has completed the training module and accordinly notifies the user to do that. .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/school/2022/01/31/projectblog.html",
            "relUrl": "/school/2022/01/31/projectblog.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "College Board Questions",
            "content": "My errors . Question 1 . . In this question, the answer I chose did not correspond with the rules of the program. X=3, Y=2 Z=1 does satisfy the conditions of the program . . In this question, the answer I chose did satisfy the conditons since if you go thru the program, it leads to that destination. The question asked which one DIDNT satisfy the conditons, and if you go thru the steps of the program, you can see that D does not satisfy the conditons of this program . . In this question, I didnt look at all of the choices that I had. I only picked the one. . . In this question, I had misread the signs. If I had chosen J&gt;7, it would result in a infitie loop because there are an infinte number of numbers after 7 . . ## . . . ## . Question 1 .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/01/31/collegeboardquestions.html",
            "relUrl": "/markdown/2022/01/31/collegeboardquestions.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "5.1 5.2 blog NOTES",
            "content": "5.1, 5.2 blog . Think of three beneficial effects of your projects based on 5.1 Unit . It can help users with learning market patterns and knowing when to invest | It simulates real results without losing any money while day trading | It provides data of the stock market directly to the | . Think of a potential harmful effect of your project. . Our platform is just a simulation so the user does not gain nor lose money. If they get too confident it could lead to them losing money when day trading with real money. | . Answer with an opinion and learnings so far this year: What are pros/cons on internet blockers at router and lack of admin password on lab machines at school? . Pros: . It restricts users from innapporpriate websites | Restricts the user from accessing malicous websites | keeps the user focused from distractions that could come from unrestricted sites | . Cons: . Sometimes it restricts websites could be used for academic purposes | . What concerns do you have personally about the digital divide? For yourself or for others. . For others, they may not have the same resources that others have, which could lead them to be less educated. Also, everybody deserves to know the truth, because without it we can be controlled by others. | .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/school/2022/01/29/5.1,-5.2-blog.html",
            "relUrl": "/school/2022/01/29/5.1,-5.2-blog.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "GradingPart3",
            "content": "Student 1 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 0 | 1 | The student does not show that he has a clear understanding of the program function and purpose and does not show the program function. | Data Abstraction|0 | 0 |The program does not show that the student has a an understanding of data abstraction| | Managing Complexity| 0 | 0 | The program does not understanding of managing complexity. When it comes to implementing the complexity, there is not enough material | | Procedural Abstraction | 0 | 0 | The program does not show understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 1 | 0 | The program shows an understanding of algorithm implementation and implemets it well. | Testing | 1| 1|the student clearly demonstrates the program and its code| . Student 2 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 1 | The student shows that he has a clear understanding of the program function and†ns purpose and does show the program function. | Data Abstraction|1 | 1 |The program shows that the sudent has an understanding of data abstraction and it to improve the program’s functionality. | | Managing Complexity| 1| 1 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 1 | 1 | The program shows a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 1 | 1 | The program shows an understanding of algorithm implementation and implemets it well. | Testing | 1| 1| the student clearly demonstrates the program and its code| . Student 3 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 1 | The student shows that he has a clear understanding of the program function and purpose and does show the program function. | Data Abstraction|0 | 0 |The program doess not show that the sudent has an understanding of data abstraction . | | Managing Complexity| 1 | 0 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 0 | 0 | The program does not show a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 0 | 0 | The student gives a minimal description of the program algorithm | Testing | 0| 0| The student does not clearly demonstrates the program and its code| . Student 4 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 1 | The student shows that he has a clear understanding of the program function and purpose and does show the program function. | Data Abstraction|1 | 1 |The program shows that the sudent has a an understanding frejuyof data abstraction and it to improve the program’s functionality. | | Managing Complexity| 1 | 1 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 1 | 1 | The program shows a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 1 | 0 | The student gives a minimal description of the program algorithm| | Testing | 1| 1| The student clearly demonstrates the program and its code| .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/01/23/GradingPart3.html",
            "relUrl": "/markdown/2022/01/23/GradingPart3.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "GradingPart2",
            "content": "Student 1 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 0 | The student shows that he has a clear understanding of the program function and purpose and does show the program function. | Data Abstraction|1 | 0 |The program shows that the student has a an understanding of data abstraction and it to improve the program’s functionality. | | Managing Complexity| 1 | 0 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 1 | 0 | The program shows a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 1 | 0 | The program shows a understanding of algorithm implementation and implemets it well. | Testing | 1| 1| the student clearly demonstrates the program and its code| . Student 2 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 1 | The student shows that he has a clear understanding of the program function and†ns purpose and does show the program function. | Data Abstraction|1 | 1 |The program shows that the sudent has a an understanding of data abstraction and it to improve the program’s functionality. | | Managing Complexity| 1 | 1 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 1 | 1 | The program shows a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 1 | 1 | The program shows a understanding of algorithm implementation and implemets it well. | Testing | 1| 1| the student clearly demonstrates the program and its code| . Student 3 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 0 | The student shows that he has a clear understanding of the program function and purpose and does show the program function. | Data Abstraction|1 | 0 |The program shows that the sudent has a an understanding of data abstraction and it to improve the program’s functionality. | | Managing Complexity| 1 | 0 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 1 | 0 | The program shows a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 0 | 0 | The code is not complex | Testing | 1| 1| the student clearly demonstrates the program and its code| . Student 4 . | Category | My score | College board | Commentary | | ———— | ————- | ————- | ————- | | Program Purpose and Function| 1 | 0 | The student shows that he has a clear understanding of the program function and purpose and does show the program function. | Data Abstraction|1 | 0 |The program shows that the sudent has a an understanding frejuyof data abstraction and it to improve the program’s functionality. | | Managing Complexity| 1 | 0 | The program shows a good understanding of managing complexity. When it comes to implementing the complexity, the student does that well | | Procedural Abstraction | 1 | 0 | The program shows a base understanding of procedural abstraction and implements it to improve the program’s functionality. | | Algorithm Implementation | 0 | 0 | There is not much algorithm implementation, I just see code | 1| 1| the student clearly demonstrates the program and its code| .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/markdown/2022/01/23/GradingPart2.html",
            "relUrl": "/markdown/2022/01/23/GradingPart2.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "CPT blog",
            "content": "Student 1 Reflection . Category Student 1 Student 2 College board Student grade Commentary     . Purpose Function | Does describe the overall purpose of the program, as well as the functionality. The input and outpit of the program was described in the video | Does describe the overall purpose of the program, as well as the functionality. The input and outpit of the program was described in the video. | 0/1 | 1/1 | Content Cell |   |   | . Data Abstraction | They do show the the two program code segments, and shos how data has been stores as well. The data in the list relates to the purpose of fullfilling the program purpose | They do show the the two program code segments, and shos how data has been stores as well. The data in the list relates to the purpose of fullfilling the program purpose | 1/1 | 1/1 | Content Cell |   |   | . Managing Complexity | Including a program code segment that shows the complexity in the program | Including a program code segment that shows the complexity in the program | 1/1 | 0/1 | Content Cell |   |   | . Procedure Abstraction | The program includes the necessary segments, one where it is shown that the student develops the procedure with at least one parameter, and one that shows what the procedure is called | The program includes the necessary segments, one where it is shown that the student develops the procedure with at least one parameter, and one that shows what the procedure is called | 1/1 | 1/1 | Content Cell |   |   | . Algorithm Implementation | All of the program code is there, sequencing, selection, iteration, and is explained in detailed steps how the algorithm works. | All of the program code is there, sequencing, selection, iteration, and is explained in detailed steps how the algorithm works | 1/1 | 1/1 | Content Cell |   |   | . Testing | All conditions were tested, and the result was outputted correctly based on the conditions given | All conditions were tested, and the result was outputted correctly based on the conditions given |   | 1/1 | 1/1 | Content Cell | Content Cell | . Student 1 reflection: . - The program served its main served its main purpose, which was giving information of a triangle based on the input. - They lost points on the first critera from collegeboard because even though they showed the input and output, they did not specify the program purpose, instead they described the code/function itself - They lost points on the create preformance task because they did not include an accurate explanition of how the program could be written differently without lists, and how lists are beneficial to the program . Student 2 . The student earned points on all of the criteria. The program’s main purpose is to serve as entertainment for the user, and the program allows the user to place and remove tiles on a grid, which are progressed programatticly by the rules of Game of life, quoting collegeboard. | | .",
            "url": "https://kushsirohi123.github.io/kushsirohirepo/2022/01/23/GradingPart1.html",
            "relUrl": "/2022/01/23/GradingPart1.html",
            "date": " • Jan 23, 2022"
        }
        
    
  

  
  
      ,"page0": {
          "title": "APCSP vocab",
          "content": "md - markdown . .ipynb - jupyter extention . HTML - Hyper Text Markup Language . if statement - conditional statement that decided wether something runs. Very versatile . Def- makes a function, which is essentially resuable code . API - application programming interface, it is a way for the computer for communicate with each other .",
          "url": "https://kushsirohi123.github.io/kushsirohirepo/_pages/2022-08-28-APCSPvocab.html",
          "relUrl": "/_pages/2022-08-28-APCSPvocab.html",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Submenu",
          "content": "| Table | JavaScript | .",
          "url": "https://kushsirohi123.github.io/kushsirohirepo/_pages/2022-09-25-submenu.html",
          "relUrl": "/_pages/2022-09-25-submenu.html",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "About Me",
          "content": "I am a 14 year old sophmore attending del norte high school. I was born in India, and moved to the US when I was 8. Ive been coding for a couple months prior to this class, and im excited to learn new aspects about programming. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kushsirohi123.github.io/kushsirohirepo/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushsirohi123.github.io/kushsirohirepo/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}